---
title: GPT-4o使用详细教程，免费向所有用户提供GPT-4级别的AI,可以实时对音频、视觉和文本进行推理
description: GPT-4o（“o”代表“omni”）是迈向更自然的人机交互的一步——它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出
meta: 
  - 
    name: description
    content: GPT-4o（“o”代表“omni”）是迈向更自然的人机交互的一步——它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出
  - 
    name: keywords
    content: GPT-4o
tags: 
  - GPT-4o
permalink: /collect/gpt-4o/
categories: 
  - gpt-4o
date: 2024-05-14 09:29:42
---
# GPT-4o使用详细教程，免费向所有用户提供GPT-4级别的AI,可以实时对音频、视觉和文本进行推理

OpenAI 2024 Spring推出 GPT-4o，这是OpenAI的新旗舰模型，可以实时对音频、视觉和文本进行推理。

PS：如果想成为GPT-4o付费用户点击: [WildCard不开卡GPT4随心用](https://bewildcard.com/i/GPT310)  邀请码 **GPT310**，直接免 2 美元的开卡费，虚拟卡开好之后，用支付宝充值进行ChatGpt升级，或者不开卡，直接使用chatgpt[随心用](https://aliyuntm.com/collect/chatgpt-free/)，只需手机号注册就能使用，等其他海外订阅服务。


GPT-4o（“o”代表“omni”）是迈向更自然的人机交互的一步——它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 320 毫秒，与人类的响应时间相似在一次谈话中。

它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显著提高，同时 API 的速度也更快，成本降低了 50%。与现有模型相比，GPT-4o 在视觉和音频理解方面尤其出色。


## 模型能力

在 GPT-4o 之前，您可以使用语音模式与 ChatGPT 对话，平均延迟为 2.8 秒 (GPT-3.5) 和 5.4 秒 (GPT-4)。为了实现这一目标，语音模式是由三个独立模型组成的管道：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将该文本转换回音频。这个过程意味着主要智能来源GPT-4丢失了大量信息——它无法直接观察音调、多个说话者或背景噪音，也无法输出笑声、歌唱或表达情感。

借助 GPT-4o，OpenAI跨文本、视觉和音频端到端地训练了一个新模型，这意味着所有输入和输出都由同一神经网络处理。由于 GPT-4o 是第一个结合所有这些模式的模型，因此OpenAI仍然只是浅尝辄止地探索该模型的功能及其局限性。


## 模型评估
根据传统基准测试，GPT-4o 在文本、推理和编码智能方面实现了 GPT-4 Turbo 级别的性能，同时在多语言、音频和视觉功能上设置了新的高水位线。

**文本评价：** GPT-4o 在 0-shot COT MMLU（常识问题）上创下了 88.7% 的新高分。此外，在传统的5-shot no-CoT MMLU上，GPT-4o创下了87.2%的新高分

![](https://hlplch.aliyuntm.com/chatgpt/5149.png)

**音频 ASR 性能：**  GPT-4o 比 Whisper-v3 显著提高了所有语言的语音识别性能，特别是对于资源匮乏的语言。

![](https://hlplch.aliyuntm.com/chatgpt/5150.png)

**音频翻译性能：**  GPT-4o 在语音翻译方面树立了新的最先进水平，并且在 MLS 基准测试中优于 Whisper-v3。

![](https://hlplch.aliyuntm.com/chatgpt/5151.png)

**M3Exam：**  M3Exam 基准测试既是多语言评估也是视觉评估，由来自其他国家标准化测试的多项选择题组成，有时还包括图形和图表。在所有语言的基准测试中，GPT-4o 都比 GPT-4 更强。 

![](https://hlplch.aliyuntm.com/chatgpt/5152.png)

**视觉理解评估：**  GPT-4o 在视觉感知基准上实现了最先进的性能。所有视觉评估都是 0-shot，其中 MMMU、MathVista 和 ChartQA 作为 0-shot CoT。
![](https://hlplch.aliyuntm.com/chatgpt/5153.png)

## 可用性

GPT-4o 是OpenAI突破深度学习界限的最新举措，这次是朝着实用性的方向发展。在过去的两年里，OpenAI花费了大量的精力来提高堆栈每一层的效率。作为这项研究的第一个成果，OpenAI能够更广泛地提供 GPT-4 级别模型。 GPT-4o 的功能将迭代推出。

GPT-4o 的文本和图像功能今天开始在 ChatGPT 中推出。OpenAI正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。OpenAI将在未来几周内在 ChatGPT Plus 中推出新版本的语音模式 GPT-4o alpha。

开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。与 GPT-4 Turbo 相比，GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍。OpenAI计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持。


## ChatGPT免费用户，可以访问的功能

OpenAI官博还介绍了，ChatGPT免费用户可以访问新模型加持下的功能，包括：
- 体验GPT-4级别的智能
- 从联网后的模型得到响应
- 分析数据并创建图表
- 畅聊你拍的照片
- 上传文件以帮助总结、撰写或分析
- 发现和使用GPTs和GPT Store
- 用记忆构建更有用的体验

## 免费向所有人提供GPT-4级别的AI

  这款全新的AI模型，免费向所有人提供GPT-4级别的AI。

  现在，进入ChatGPT页面，Plus用户可以抢先体验「最新、最先进的模型」GPT-4o。
  ![](https://hlplch.aliyuntm.com/chatgpt/5154.png)
  ![](https://hlplch.aliyuntm.com/chatgpt/5155.png)

  这个带着光环登场的模型，其最大意义就在于，把GPT-4级别的智能，带给了OpenAI的每一位用户！

  **从此以后，无论你是付费用户，还是免费用户，都能通过它体验GPT-4了。**

  唯一不同的是，ChatGPT Plus的消息限制是免费用户的5倍。

  并且，GPT-4o不仅提供与GPT-4同等程度的模型能力，推理速度还更快，还能提供同时理解文本、图像、音频等内容的多模态能力。

## WildCard不开卡GPT4随心用

  如果想成为GPT-4o付费用户点击: [WildCard不开卡GPT4随心用](https://bewildcard.com/i/GPT310)  邀请码 **GPT310**，直接免 2 美元的开卡费，虚拟卡开好之后，用支付宝充值进行ChatGpt升级，或者不开卡，直接使用chatgpt[随心用](https://aliyuntm.com/collect/chatgpt-free/)，只需手机号注册就能使用，等其他海外订阅服务。

  在GPT-4o发布之前，通过语音模式（Voice Mode）与ChatGPT对话，平均延迟为2.8秒（GPT-3.5）和5.4秒（GPT-4）。

  它可以跨越语音、文本、视觉多种形式，直接进行推理！

  GPT-4o是OpenAI首个端到端训练的跨越文本、视觉和音频的新模型，意味着所有输入和输出都由相同的神经网络处理。

  这就会彻底颠覆ChatGPT 1亿用户的工作和生活。

  不仅如此，由于GPT-4o是「原生的多模态」，自然地集成了语言、视觉和音频等多种能力。

  用户可以上传各种图片、视频，以及包含图片和文字的文档，讨论其中的内容。

  以上就是这次OpenAI春季发布会的全部内容了。

  在这场发布会之前，无数人曾在猜测，OpenAI到底会发一些什么王炸，什么才能配得上奥特曼口中的"magic"。

  ![](https://hlplch.aliyuntm.com/chatgpt/5157.png)

  那现在，OpenAI做到了，他们用GPT-4o依然证明了，他们是AI届的王者。
  
**参考**：
[https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)